---
title: "第9章: RNN 与 CNN"
date: 2020-09-09 19:46:20 +0800
layout: single
toc: true
lang: zh
sidebar: {nav: "sidebar_zh"}
header:
  teaser: /assets/images/ch09.png
---

## 80. 将单词转换为数字ID 
试将51问创建的数据中的词汇转为数字id的表示。
将每个出现了两次及以上的单词赋予一个数字id，使得最频繁出现的单词的id为`1`，词频第二高的id为`2`，以此类推。
按照上述要求实现一个函数，对给定的单词返回一个id。特别地，如果一个单词出现次数少于两次则返回`0`。

## 81. 使用RNN进行预测
设有一个含有数字ID的序列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$，其中每个元素对应某句话中的单词。
其中，$$T$$是该序列（句子）中单词的数量，而$$x_t \in \mathbb{R}^{V}$$表示独热编码向量（$$V$$是词表中单词的数量）。
使用一个循环神经网络(RNN, Recurrent Neural Network)来建构一个模型，使得它能够完成对一个包含单词ID的序列$$\boldsymbol{x}$$给出其所属分类$$y$$的预测任务。

$$
\overrightarrow{h}_0 = 0, \\
\overrightarrow{h}_t = {\rm \overrightarrow{RNN}}(\mathrm{emb}(x_t), \overrightarrow{h}_{t-1}), \\
y = {\rm softmax}(W^{(yh)} \overrightarrow{h}_T + b^{(y)}),
$$

其中，$$\mathrm{emb}(x) \in \mathbb{R}^{d_w}$$表示词嵌入（一个能够将稀疏的one-hot编码向量映射到低维的密集向量表示的函数）。$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}$$是时序上$$t$$时刻的隐藏状态(hidden state)。此外，$${\rm \overrightarrow{RNN}}(x,h)$$是一个RNN单元，它将当前时刻$$t$$的输入$$x$$与前一时刻的隐藏状态$$h$$，共同作为元素计算下一个时刻的隐藏状态。矩阵$$W^{(yh)} \in \mathbb{R}^{L \times d_h}$$ 被用作线性变换以使用隐藏状态进行分类预测，$$b^{(y)} \in \mathbb{R}^{L}$$则是偏置项（$$d_w$$, $$d_h$$ 和 $$L$$分别表示词向量、隐藏状态、分类数量的维度）。RNN单元$${\rm \overrightarrow{RNN}}(x,h)$$中可以考虑有各种不同的构成，一个最简单的例子如下：

$$
{\rm \overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)}),
$$

其中，$$W^{(hx)} \in \mathbb{R}^{d_h \times d_w}，W^{(hh)} \in \mathbb{R}^{d_h \times d_h}, b^{(h)} \in \mathbb{R}^{d_h}$$是RNN单元的参数，$$g$$代表激活函数（例如$$\tanh$$ 或者 ReLU）。

本问题中并不需要实行模型训练，只需要对模型参数进行随机初始化并计算分类$$y$$即可。对于维度，考虑使用下述设定$$d_w = 300, d_h=50$$（在下面的问题中保持与本问同样的设定）。

## 82. 使用随机梯度下降法进行训练

使用随机梯度下降训练法(SGD，Stochastic Gradient Descent)对问题81中的模型进行训练。在训练期间，打印出在训练集与验证集上的损失与正确率，并考虑使用适当的策略对模型训练进行终止（例如训练10个Epochs后停止）。

## 83. Mini-batch, GPU训练

修改82问中的代码，使其以每$$B$$个样本为mini-batch单位来计算损失与梯度，并进行学习（可以适当选择$$B$$的值），并在GPU上进训练。

## 84. 使用预训练词向量

使用预训练词向量（例如, 约使用1,000亿词语训练的[Google News embeddings](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）来初始化词嵌入 $$\mathrm{emb}(x)$$ 并进行模型训练。

## 85. 双向RNN与多层RNN
使用前向和反向RNN对输入文本进行编码，并训练模型。

$$
\overleftarrow{h}_{T+1} = 0, \\
\overleftarrow{h}_t = {\rm \overleftarrow{RNN}}(\mathrm{emb}(x_t), \overleftarrow{h}_{t+1}), \\
y = {\rm softmax}(W^{(yh)} [\overrightarrow{h}_T; \overleftarrow{h}_1] + b^{(y)})
$$

其中，$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}, \overleftarrow{h}_t \in \mathbb{R}^{d_h}$$是由前向和反向RNN在时刻$$t$$上产生的隐藏状态向量，$${\rm \overleftarrow{RNN}}(x,h)$$使用输入$$x$$与下一时刻的隐藏状态$$h$$计算前一时刻的隐藏状态，矩阵$$W^{(yh)} \in \mathbb{R}^{L \times 2d_h}$$用来从隐藏状态进行分类预测，$$b^{(y)} \in \mathbb{R}^{L}$$是偏置项，$$[a; b]$$表示两个向量$$a$$与$$b$$的连接。

此外, 对双向RNN进行多层化并重复该实验。

### 86. 卷积神经网络 (CNN)

考虑一个单词序列 $$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$，$$T$$是序列的长度。
此外， $$x_t \in \mathbb{R}^{V}$$是将词汇以ID表示并进行独热编码的向量（$$V$$是词表中单词的数量）。
实现一个卷积神经网络(CNN, Convolutional Neural Network)来建构一个模型，使用它对序列$$\boldsymbol{x}$$预测其所属分类$$y$$。

CNN的设定如下： 

+ 词嵌入维度: $$d_w$$
+ 卷积Filter大小: 3 tokens
+ 卷积Stride: 1 token
+ 卷积Padding: 启用
+ 每次卷积操作后产生的向量维度: $$d_h$$
+ 在每次卷积操作后使用最大池化(max-pooling)并将输入句子表示为$$d_h$$维的向量

也就是说, 在$$t$$时刻的特征向量$$p_t \in \mathbb{R}^{d_h}$$可以使用下述公式进行计算:

$$
p_t = g(W^{(px)} [\mathrm{emb}(x_{t-1}); \mathrm{emb}(x_t); \mathrm{emb}(x_{t+1})] + b^{(p)})
$$

其中, $$W^{(px)} \in \mathbb{R}^{d_h \times 3d_w}$$, $$b^{(p)} \in \mathbb{R}^{d_h}$$ 是CNN的参数, $$g$$是激活函数(例如$$\tanh$$, ReLU等), $$[a; b; c]$$表示向量$$a,b,c$$的连接。矩阵 $$W^{(px)}$$的列数为$$3d_w$$的原因在于，需要对3个连接后的词向量进行线性变换。

在最大池化操作中，取所有时刻的每个维度的最大值，获得输入文档的特征向量$$c \in \mathbb{R}^{d_h}$$。假设$$c[i]$$代表向量$$c$$的第$$i$$个维度，最大池化通过以下公式进行计算：

$$
c[i] = \max_{1 \leq t \leq T} p_t[i]
$$

最后，对输入文档的特征向量，使用矩阵$$W^{(yc)} \in \mathbb{R}^{L \times d_h}$$ 及其偏置$$b^{(y)} \in \mathbb{R}^{L}$$进行线性变换后应用softmax激活函数，预测分类其所属分类$$y$$：

$$
y = {\rm softmax}(W^{(yc)} c + b^{(y)})
$$

此外，不训练模型，尝试使用随机初始化后的权重矩阵来计算$$y$$。

## 87. 使用随机梯度下降法进行CNN训练

使用随机梯度下降法(SGD), 对86问中构建的模型进行训练。在训练期间，打印出在训练集与验证集上的损失与正确率，并考虑使用适当的策略对模型训练进行终止（例如训练10个Epochs后停止）。

## 88. 超参数调节

修改问题85或者87中的代码，通过调节神经网络的参数量与超参数，构建一个高性能的分类器。

## 89. 使用预训练的语言模型进行迁移学习

基于预训练的语言模型(如[BERT](https://github.com/google-research/bert)等)，构建一个能对新闻文章进行分类的模型。

